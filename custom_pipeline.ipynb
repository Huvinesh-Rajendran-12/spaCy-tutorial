{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "First, write a function that takes a Doc as input, performs neccessary tasks and returns a new Doc. Then, add this function to the spacy pipeline through nlp.add_pipe() method.\r\n",
    "\r\n",
    "The parameters of add_pipe you have to provide :\r\n",
    "\r\n",
    "    component : You have to pass the function_name as input . This serves as our component\r\n",
    "    name : You can assign a name to the component. The component can be called using this name. If you don’t provide any ,the function_name will be taken as name of the component\r\n",
    "    first,last : If you want the new component to be added first or last ,you can setfirst=True or last=True accordingly.\r\n",
    "    before , after : If you want to add the component specifically before or after another component , you can use these arguments.\r\n",
    "\r\n",
    "Note that you can set only one among first, last, before, after arguments, otherwise it will lead to error."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import spacy \r\n",
    "from spacy.language import Language"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# Define the custom component that prints the doc length and named entities.\r\n",
    "@Language.component(\"my_custom_component\")\r\n",
    "def my_custom_component(doc):\r\n",
    "    doc_length = len(doc)\r\n",
    "    print(' The no of tokens in the document ', doc_length)\r\n",
    "    named_entity=[token.label_ for token in doc.ents]\r\n",
    "    print(named_entity)\r\n",
    "    # Return the doc\r\n",
    "    return doc"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# Load the small English model\r\n",
    "nlp = spacy.load(\"en_core_web_sm\")\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "nlp.add_pipe(\"my_custom_component\",after='ner')\r\n",
    "print(nlp.pipe_names)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner', 'my_custom_component']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# Call the nlp object on your text\r\n",
    "doc = nlp(\" The Hindu Newspaper has increased the cost. I usually read the paper on my way to Delhi railway station \")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " The no of tokens in the document  21\n",
      "['ORG', 'GPE']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "from spacy.matcher import PhraseMatcher"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\r\n",
    "matcher = PhraseMatcher(nlp.vocab)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "book_names = ['Pride and prejudice','Mansfield park','The Tale of Two cities','Great Expectations']\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# Creating pattern - list of docs through nlp.pipe() to save time\r\n",
    "book_patterns = list(nlp.pipe(book_names))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# Adding the pattern to the matcher\r\n",
    "matcher.add(\"identify_books\", None, *book_patterns)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can go ahead and write the function for custom pipeline. This function shall use the matcher to find the patterns in the doc , add it to doc.ents and return the doc. Note that when matcher is applied on a Doc , it returns a tuple containing (match_id,start,end). You can extract the span using the start and end indices and store it in doc.ents"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# Import Span to slice the Doc\r\n",
    "from spacy.tokens import Span"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "# Define the custom pipeline component\r\n",
    "@Language.component(\"identify_books\")\r\n",
    "def identify_books(doc):\r\n",
    "    # Apply the matcher to YOUR doc\r\n",
    "    matches = matcher(doc)\r\n",
    "    # Create a Span for each match and assign them under label \"BOOKS\"\r\n",
    "    spans = [Span(doc, start, end, label=\"BOOKS\") for match_id, start, end in matches]\r\n",
    "    # Store the matched spans in doc.ents\r\n",
    "    doc.ents = spans\r\n",
    "    return doc"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Your custom component identify_books is also ready. Final step is to add this to the spaCy’s pipeline through nlp.add_pipe(identify_books) method."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "# Adding the custom component to the pipeline after the \"ner\" component\r\n",
    "nlp.add_pipe(\"identify_books\", after=\"ner\")\r\n",
    "print(nlp.pipe_names)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner', 'identify_books']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "# Calling the nlp object on the text\r\n",
    "doc = nlp(\"The library has got several new copies of Mansfield park and Great Expectations . I have filed a suggestion to buy more copies of The Tale of Two cities \")\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "# Printing entities and their labels to verify\r\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('Mansfield park', 'BOOKS'), ('Great Expectations', 'BOOKS'), ('The Tale of Two cities', 'BOOKS')]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "From above output , you can verify that the patterns have been identified and successfully placed under category “BOOKS”.\r\n",
    "\r\n",
    "That’s how custom pipelines are useful in various situations."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit ('spacy': venv)"
  },
  "interpreter": {
   "hash": "da7a3f20bd736895ed2701e5b4df063f6abe2bef9c68d91259d0aafe078167d8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}