{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "When you call the nlp object on spaCy, the text is segmented into tokens to create a Doc object. Following this, various process are carried out on the Doc to add the attributes like POS tags, Lemma tags, dependency tags,etc..\r\n",
    "\r\n",
    "This is referred as the Processing Pipeline"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The processing pipeline consists of components, where each component performs it’s task and passes the Processed Doc to the next component. These are called as pipeline components.\r\n",
    "\r\n",
    "spaCy provides certain in-built pipeline components. Let’s look at them.\r\n",
    "\r\n",
    "The built-in pipeline components of spacy are :\r\n",
    "\r\n",
    "    Tokenizer : It is responsible for segmenting the text into tokens are turning a Doc object. This the first and compulsory step in a pipeline.\r\n",
    "    Tagger : It is responsible for assigning Part-of-speech tags. It takes a Doc as input and createsDoc[i].tag\r\n",
    "    DependencyParser : It is known as parser. It is responsible for assigning the dependency tags to each token. It takes a Doc as input and returns the processed Doc\r\n",
    "    EntityRecognizer : This component is referred as ner. It is responsible for identifying named entities and assigning labels to them.\r\n",
    "    TextCategorizer : This component is called textcat. It will assign categories to Docs.\r\n",
    "    EntityRuler : This component is called * entity_ruler*.It is responsible for assigning named entitile based on pattern rules. Revisit Rule Based Matching to know more.\r\n",
    "    Sentencizer : This component is called **sentencizer** and can perform rule based sentence segmentation.\r\n",
    "    merge_noun_chunks : It is called mergenounchunks. This component is responsible for merging all noun chunks into a single token. It has to be add in the pipeline after tagger and parser.\r\n",
    "    merge_entities : It is called merge_entities .This component can merge all entities into a single token. It has to added after the ner.\r\n",
    "    merge_subtokens : It is called merge_subtokens. This component can merge the subtokens into a single token.\r\n",
    "\r\n",
    "These are the various in-built pipeline components. It is not necessary for every spaCy model to have each of the above components.\r\n",
    "\r\n",
    "After loading a spaCy model , you check or inspect what pipeline components are present."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "import spacy\r\n",
    "from spacy_langdetect import LanguageDetector \r\n",
    "from spacy.language import Language"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\r\n",
    "print(nlp.pipe_names)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# Check if pipeline component present\r\n",
    "nlp.has_pipe('textcat')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# Add new pipeline component\r\n",
    "nlp.add_pipe('textcat')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<spacy.pipeline.textcat.TextCategorizer at 0x153c164e400>"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "nlp.pipe_names"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['tok2vec',\n",
       " 'tagger',\n",
       " 'parser',\n",
       " 'attribute_ruler',\n",
       " 'lemmatizer',\n",
       " 'ner',\n",
       " 'textcat']"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "nlp.add_pipe('textcat',before='ner')\r\n",
    "nlp.pipe_names"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['tok2vec',\n",
       " 'tagger',\n",
       " 'parser',\n",
       " 'attribute_ruler',\n",
       " 'lemmatizer',\n",
       " 'textcat',\n",
       " 'ner']"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "# Printing the components initially\r\n",
    "print(' Pipeline components present initially')\r\n",
    "print(nlp.pipe_names)\r\n",
    "\r\n",
    "# Removing a pipeline component and printing \r\n",
    "nlp.remove_pipe(\"textcat\")\r\n",
    "print('After removing the textcat pipeline')\r\n",
    "print(nlp.pipe_names)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " Pipeline components present initially\n",
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'textcat', 'ner']\n",
      "After removing the textcat pipeline\n",
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "# Renaming pipeline components\r\n",
    "nlp.rename_pipe(old_name='ner',new_name='my_custom_ner')\r\n",
    "nlp.pipe_names"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['tok2vec',\n",
       " 'tagger',\n",
       " 'parser',\n",
       " 'attribute_ruler',\n",
       " 'lemmatizer',\n",
       " 'my_custom_ner']"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The name of component changed in above output.\r\n",
    "\r\n",
    "spaCy also allows you to create your own custom pipelines. We shall discuss more on this later. When you have to use different component in place of an existing component, you can use nlp.replace_pipe() method."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "nlp.replace_pipe"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<bound method Language.replace_pipe of <spacy.lang.en.English object at 0x00000153BD91E610>>"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit ('spacy': venv)"
  },
  "interpreter": {
   "hash": "da7a3f20bd736895ed2701e5b4df063f6abe2bef9c68d91259d0aafe078167d8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}